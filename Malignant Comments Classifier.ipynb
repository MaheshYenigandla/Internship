{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing warning library to avoid any warnings\n",
    "import pandas as pd # for data wrangling purpose\n",
    "import numpy as np # Basic computation library\n",
    "import seaborn as sns # For Visualization \n",
    "import matplotlib.pyplot as plt # ploting package\n",
    "%matplotlib inline\n",
    "import warnings # Filtering warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5604283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1ef545",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No. of Rows :',df.shape[0])\n",
    "print('No. of Columns :',df.shape[1])\n",
    "pd.set_option('display.max_columns',None) # This will enable us to see truncated columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e204b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_series().groupby(df.dtypes).groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d31a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft=pd.read_csv('test.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5700828",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No. of Rows :',dft.shape[0])\n",
    "print('No. of Columns :',dft.shape[1])\n",
    "pd.set_option('display.max_columns',None) # This will enable us to see truncated columns\n",
    "dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dbd5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c251ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.isnull().sum().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761cf9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_text'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91495b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Categorical = ['malignant', 'highly_malignant', 'rude', 'threat', 'abuse', 'loathe']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ba1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns[2:]:\n",
    "    print('Value Counts of',i)\n",
    "    print(df[i].value_counts())\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d200f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting countplot for all the features\n",
    "categories=df.columns[2:]\n",
    "plt.figure(figsize=(15,20),facecolor='white')\n",
    "plotnumber=1\n",
    "for col in categories:\n",
    "    if plotnumber<=6:\n",
    "        ax=plt.subplot(3,2,plotnumber)\n",
    "        sns.countplot(df[col])\n",
    "        plt.xlabel(col,fontsize=20)\n",
    "        plt.xticks(fontsize=16,fontweight ='bold')\n",
    "        plt.yticks(fontsize=16,fontweight ='bold')\n",
    "    plotnumber += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the percentage of the comments\n",
    "none = df[(df['malignant']!=1) & (df['highly_malignant']!=1) & (df['rude']!=1) & \n",
    "                            (df['threat']!=1) & (df['abuse']!=1) & (df['loathe']!=1)]\n",
    "percent=len(none)/len(df)*100\n",
    "print('Percentage of good/neutral comments = ',percent)\n",
    "print('Percentage of negative comments = ', (100-percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4459b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_count = df.iloc[:,2:].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23854975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a chart with the following size\n",
    "plt.figure(figsize=(14,9))\n",
    "\n",
    "# Plot a bar chart using the index (category values) and the count of each category.\n",
    "ax = sns.barplot(data_count.index, data_count.values)\n",
    "\n",
    "plt.title(\"No. of Comments per Class\", fontsize=20, fontweight='bold')\n",
    "plt.ylabel('No. of Occurrences', fontsize=18,fontweight='bold')\n",
    "plt.xlabel('Comment Categories', fontsize=18,fontweight='bold')\n",
    "plt.xticks(fontsize=16,fontweight ='bold')\n",
    "plt.yticks(fontsize=16,fontweight ='bold')\n",
    "rects = ax.patches\n",
    "labels = data_count.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436ec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing normal comments and bad comments using count plot\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(18,9))\n",
    "\n",
    "for i in range(2):\n",
    "    sns.countplot(data=df[df.columns[2:]][df[df.columns[2:]]==i], ax=ax[i])\n",
    "    if i == 0:\n",
    "        ax[i].set_title(\"Count Plot for Normal Comments\\n\", fontsize=18, fontweight='bold')\n",
    "    else:\n",
    "        ax[i].set_title(\"Count Plot for Bad Comments\\n\", fontsize=18, fontweight='bold')\n",
    "        \n",
    "    ax[i].set_xticklabels(df.columns[2:], rotation=90, ha=\"right\", fontsize=14, fontweight='bold')\n",
    "    p=0\n",
    "    for prop in ax[i].patches:\n",
    "        count = prop.get_height()\n",
    "        s = f\"{count} ({round(count*100/len(df),2)}%)\"\n",
    "        ax[i].text(p,count/2,s,rotation=90, ha=\"center\", fontweight=\"bold\")\n",
    "        p += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3cdea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the label distribution of comments using pie chart\n",
    "comments_labels = ['malignant', 'highly_malignant', 'rude', 'threat', 'abuse', 'loathe']\n",
    "df_distribution = df[df.columns[2:]].sum()\\\n",
    "                            .to_frame()\\\n",
    "                            .rename(columns={0: 'count'})\\\n",
    "                            .sort_values('count')\n",
    "\n",
    "df_distribution.plot.pie(y = 'count', title = 'Label distribution over comments',\n",
    "                         autopct='%2.2f', figsize = (8,8))\\\n",
    "                            .legend(loc='center left', bbox_to_anchor=(1.3, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772aa814",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_len = df.comment_text.str.len()\n",
    "df.comment_text.str.len().median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5953563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of comments length\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(10,7))\n",
    "comment_len = df.comment_text.str.len()\n",
    "sns.distplot(comment_len, bins=20, color = 'blue')\n",
    "\n",
    "plt.title(\"Distribution of Comment Length\", fontsize=20, fontweight='bold')\n",
    "plt.ylabel('Density', fontsize=18,fontweight='bold')\n",
    "plt.xlabel('Length of Comment', fontsize=18,fontweight='bold')\n",
    "plt.xticks(fontsize=16,fontweight ='bold')\n",
    "plt.yticks(fontsize=16,fontweight ='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98143405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking correlation of the dataset\n",
    "corr=df.corr()  \n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc758d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting heatmap for visualizing the correlation\n",
    "plt.figure(figsize=(15, 10))\n",
    "corr = df.corr() # corr() function provides the correlation value of each column\n",
    "sns.heatmap(corr, linewidth=0.5, linecolor='black', fmt='.0%', cmap='YlGn_r', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3529e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As ID is not much important, we can drop from the dataset\n",
    "df.drop('id',axis=1,inplace=True)\n",
    "dft.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8069a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the count of labels\n",
    "df['label']=df[comments_labels].sum(axis=1)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(df['label'], palette='coolwarm')\n",
    "plt.title('Counting of the labels',fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column 'length_before_cleaning' in training dataset\n",
    "# It represents the length of the each comment respectively in a column 'comment_text' \n",
    "df['length_before_cleaning'] = df['comment_text'].map(lambda comment_text: len(comment_text))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ce7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column 'length_before_cleaning' in test dataset\n",
    "# It represents the length of the each comment respectively in a column 'comment_text' \n",
    "dft['length_before_cleaning'] = dft['comment_text'].map(lambda comment_text: len(comment_text))\n",
    "dft.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc05eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Required libraries\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a534af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#Defining the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484c734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing '\\n' in comment_text\n",
    "df['comment_text'] = df['comment_text'].replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8edb513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function Definition for using regex operations and other text preprocessing for getting cleaned texts\n",
    "def clean_comments(text):\n",
    "    \n",
    "    #convert to lower case\n",
    "    lowered_text = text.lower()\n",
    "    \n",
    "    #Replacing email addresses with 'emailaddress'\n",
    "    text = re.sub(r'^.+@[^\\.].*\\.[a-z]{2,}$', 'emailaddress', lowered_text)\n",
    "    \n",
    "    #Replace URLs with 'webaddress'\n",
    "    text = re.sub(r'http\\S+', 'webaddress', text)\n",
    "    \n",
    "    #Removing numbers\n",
    "    text = re.sub(r'[0-9]', \" \", text)\n",
    "    \n",
    "    #Removing the HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    \n",
    "    #Removing Punctuations\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\_',' ',text)\n",
    "    \n",
    "    #Removing all the non-ascii characters \n",
    "    clean_words = re.sub(r'[^\\x00-\\x7f]',r'', text)\n",
    "    \n",
    "    #Removing the unwanted white spaces\n",
    "    text = \" \".join(text.split()) \n",
    "    \n",
    "    #Splitting data into words\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    \n",
    "    #Removing remaining tokens that are not alphabetic, Removing stop words and Lemmatizing the text\n",
    "    removed_stop_text = [lemmatizer.lemmatize(word) for word in tokenized_text if word not in stop_words if word.isalpha()]\n",
    "   \n",
    "    return \" \".join(removed_stop_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the above function for the column comment_text in training dataset to replace original with cleaned text\n",
    "df['comment_text'] = df['comment_text'].apply(clean_comments)\n",
    "df['comment_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e4291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column 'len_after_cleaning'\n",
    "# Representing the length of the each comment respectively in a column 'comment_text' after cleaning the text.\n",
    "df['length_after_cleaning'] = df['comment_text'].map(lambda comment_text: len(comment_text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d9ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Total length removal in train dataset\n",
    "print(\"Original Length:\", df.length_before_cleaning.sum())\n",
    "print(\"Cleaned Length:\", df.length_after_cleaning.sum())\n",
    "print(\"Total Words Removed:\", (df.length_before_cleaning.sum()) - (df.length_after_cleaning.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the above function for the column comment_text in test dataset so that we can replace original with cleaned text\n",
    "dft['comment_text'] = dft['comment_text'].apply(clean_comments)\n",
    "dft['comment_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87742513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a column 'len_after_cleaning'\n",
    "#It represents the length of the each comment respectively in a column 'comment_text' after cleaning the text\n",
    "dft['length_after_cleaning'] = dft['comment_text'].map(lambda comment_text: len(comment_text))\n",
    "dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94485ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total length removal in test dataset\n",
    "print('Original Length:',dft.length_before_cleaning.sum())\n",
    "print('Clean Length:',dft.length_after_cleaning.sum())\n",
    "print(\"Total Words Removed:\", (dft.length_before_cleaning.sum()) - (dft.length_after_cleaning.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a505129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for malignant\n",
    "df_malignant=df[(df['malignant']==1)]\n",
    "wordcloud=WordCloud(height=300,width=450,max_words=300,background_color=\"white\").generate(str(df_malignant['comment_text']))\n",
    "plt.figure(figsize=(10,10),facecolor='y')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.title(label='WORDS TAGGED AS MALIGNANT',fontdict={'fontsize':22, 'fontweight':'bold', 'color':'purple'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa48a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for highly_malignant\n",
    "df_highlymalignant=df[(df['highly_malignant']==1)]\n",
    "wordcloud=WordCloud(height=300,width=450,max_words=300,background_color=\"white\").generate(str(df_highlymalignant['comment_text']))\n",
    "plt.figure(figsize=(10,10),facecolor='y')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.title(label='WORDS TAGGED AS HIGHLY MALIGNANT',fontdict={'fontsize':22, 'fontweight':'bold', 'color':'purple'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360eabf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for rude\n",
    "df_rude=df[(df['rude']==1)]\n",
    "wordcloud=WordCloud(height=300,width=450,max_words=300,background_color=\"white\").generate(str(df_rude['comment_text']))\n",
    "plt.figure(figsize=(10,10),facecolor='y')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.title(label='WORDS TAGGED AS RUDE',fontdict={'fontsize':22, 'fontweight':'bold', 'color':'purple'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for threat\n",
    "df_threat=df[(df['threat']==1)]\n",
    "wordcloud=WordCloud(height=300,width=450,max_words=300,background_color=\"white\").generate(str(df_threat['comment_text']))\n",
    "plt.figure(figsize=(10,10),facecolor='y')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.title(label='WORDS TAGGED AS THREAT',fontdict={'fontsize':22, 'fontweight':'bold', 'color':'purple'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb26756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for abuse\n",
    "df_abuse=df[(df['abuse']==1)]\n",
    "wordcloud=WordCloud(height=300,width=450,max_words=300,background_color=\"white\").generate(str(df_abuse['comment_text']))\n",
    "plt.figure(figsize=(10,10),facecolor='y')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.title(label='WORDS TAGGED AS ABUSE',fontdict={'fontsize':22, 'fontweight':'bold', 'color':'purple'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919887fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for loathe\n",
    "df_loathe=df[(df['loathe']==1)]\n",
    "wordcloud=WordCloud(height=300,width=450,max_words=300,background_color=\"white\").generate(str(df_loathe['comment_text']))\n",
    "plt.figure(figsize=(10,10),facecolor='y')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.title(label='WORDS TAGGED AS LOATHE',fontdict={'fontsize':22, 'fontweight':'bold', 'color':'purple'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c45d60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the features into number vectors\n",
    "tf_vec = TfidfVectorizer(max_features = 2000, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Separate the input and output variables represented by X and y respectively in train data and convert them\n",
    "X = tf_vec.fit_transform(df['comment_text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6b23a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_labels= df.columns[1:7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f072c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output variables\n",
    "from scipy.sparse import csr_matrix\n",
    "Y = csr_matrix(df[output_labels]).toarray()\n",
    "\n",
    "# checking shapes of input and output variables to take care of data imbalance issue\n",
    "print(\"Input Variable Shape:\", X.shape)\n",
    "print(\"Output Variable Shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the above process for test data \n",
    "test_vec = tf_vec.fit_transform(dft['comment_text'])\n",
    "test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c0049",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-multilearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b49665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Machine learning Model library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import hamming_loss, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d0d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit, sys\n",
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40095577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training and Testing Model on our train dataset\n",
    "\n",
    "# Creating a function to train and test model\n",
    "def build_models(models,x,y,test_size=0.33,random_state=42):\n",
    "    # spliting train test data using train_test_split\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=test_size,random_state=random_state)\n",
    "    \n",
    "    # training models using BinaryRelevance of problem transform\n",
    "    for i in tqdm.tqdm(models,desc=\"Building Models\"):\n",
    "        start_time = timeit.default_timer()\n",
    "        \n",
    "        sys.stdout.write(\"\\n=======================================================================================\\n\")\n",
    "        sys.stdout.write(f\"Current Model in Progress: {i} \")\n",
    "        sys.stdout.write(\"\\n=======================================================================================\\n\")\n",
    "        \n",
    "        br_clf = BinaryRelevance(classifier=models[i][\"name\"],require_dense=[True,True])\n",
    "        print(\"Training: \",br_clf)\n",
    "        br_clf.fit(x_train,y_train)\n",
    "        \n",
    "        print(\"Testing: \")\n",
    "        predict_y = br_clf.predict(x_test)\n",
    "        \n",
    "        ham_loss = hamming_loss(y_test,predict_y)\n",
    "        sys.stdout.write(f\"\\n\\tHamming Loss  : {ham_loss}\")\n",
    "                \n",
    "        ac_score = accuracy_score(y_test,predict_y)\n",
    "        sys.stdout.write(f\"\\n\\tAccuracy Score: {ac_score}\")\n",
    "        \n",
    "        cl_report = classification_report(y_test,predict_y)\n",
    "        sys.stdout.write(f\"\\n{cl_report}\")\n",
    "        \n",
    "        end_time = timeit.default_timer()\n",
    "        sys.stdout.write(f\"Completed in [{end_time-start_time} sec.]\")\n",
    "        \n",
    "        models[i][\"trained\"] = br_clf\n",
    "        models[i][\"hamming_loss\"] = ham_loss\n",
    "        models[i][\"accuracy_score\"] = ac_score\n",
    "        models[i][\"classification_report\"] = cl_report\n",
    "        models[i][\"predict_y\"] = predict_y\n",
    "        models[i][\"time_taken\"] = end_time - start_time\n",
    "                      \n",
    "        sys.stdout.write(\"\\n=======================================================================================\\n\")\n",
    "    \n",
    "    models[\"x_train\"] = x_train\n",
    "    models[\"y_train\"] = y_train\n",
    "    models[\"x_test\"] = x_test\n",
    "    models[\"y_test\"] = y_test\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b782ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the list of models for classification purpose\n",
    "models = {\n",
    "          \"Logistic Regression\": {\"name\": LogisticRegression()},\n",
    "          \"Random Forest Classifier\": {\"name\": RandomForestClassifier()},\n",
    "          \"Support Vector Classifier\": {\"name\": LinearSVC(max_iter = 3000)},\n",
    "          \"Ada Boost Classifier\": {\"name\": AdaBoostClassifier()},\n",
    "         }\n",
    "\n",
    "# Taking one forth of the total data for training and testing purpose\n",
    "half = len(df)//4\n",
    "trained_models = build_models(models,X[:half,:],Y[:half,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f26eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb2385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmod_param = {'estimator__penalty' : ['l1', 'l2'],\n",
    "              'estimator__loss' : ['hinge', 'squared_hinge'],\n",
    "              'estimator__multi_class' : ['ovr', 'crammer_singer'],\n",
    "              'estimator__random_state' : [42, 72, 111] }\n",
    "#SVC = BinaryRelevance(classifier=LinearSVC(),require_dense=[True,True])           \n",
    "SVC = OneVsRestClassifier(LinearSVC())\n",
    "GSCV = GridSearchCV(SVC, fmod_param, cv=3,verbose = 10)\n",
    "x_train,x_test,y_train,y_test = train_test_split(X[:half,:], Y[:half,:], test_size=0.30, random_state=42)\n",
    "GSCV.fit(x_train,y_train)\n",
    "GSCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef542f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Model = OneVsRestClassifier(LinearSVC(loss='hinge', \n",
    "            multi_class='ovr', penalty='l2', random_state=42))\n",
    "\n",
    "Classifier = Final_Model.fit(x_train, y_train)\n",
    "fmod_pred = Final_Model.predict(x_test)\n",
    "fmod_acc = (accuracy_score(y_test, fmod_pred))*100\n",
    "print(\"Accuracy score for the Best Model is:\", fmod_acc)\n",
    "h_loss = hamming_loss(y_test,fmod_pred)*100\n",
    "print(\"Hamming loss for the Best Model is:\", h_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432558fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = y_test.shape[1]\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], fmod_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), fmod_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8) # used to change the output figure size\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    fpr[\"micro\"],\n",
    "    tpr[\"micro\"],\n",
    "    label=\"micro-average ROC curve (AUC = {0:0.2f})\".format(roc_auc[\"micro\"]),\n",
    "    color=\"deeppink\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    fpr[\"macro\"],\n",
    "    tpr[\"macro\"],\n",
    "    label=\"macro-average ROC curve (AUC = {0:0.2f})\".format(roc_auc[\"macro\"]),\n",
    "    color=\"navy\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(\n",
    "        fpr[i],\n",
    "        tpr[i],\n",
    "        color=color,\n",
    "        lw=2,\n",
    "        label=\"ROC curve of class {0} (AUC = {1:0.2f})\".format(i, roc_auc[i]),\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic (ROC) and Area under curve (AUC) for multiclass labels\\n\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01ab0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, multilabel_confusion_matrix\n",
    "print(\"Confusion matrix:\\n\\n\", multilabel_confusion_matrix(y_test, fmod_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715de360",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,8) # used to change the output figure size\n",
    "ax= plt.subplot()\n",
    "cm = confusion_matrix(np.asarray(y_test).argmax(axis=1), np.asarray(fmod_pred).argmax(axis=1))\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax);  # annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "# title, labels and ticks\n",
    "ax.set_title('Confusion Matrix for the Final Classification Model\\n'); \n",
    "ax.set_xlabel('Predicted labels'); ax.set_ylabel('True labels'); \n",
    "loc = plticker.MultipleLocator()\n",
    "ax.xaxis.set_major_locator(loc); ax.yaxis.set_major_locator(loc);\n",
    "ax.set_xticklabels(comments_labels); ax.set_yticklabels(comments_labels);\n",
    "plt.xticks(rotation=90); plt.yticks(rotation=0);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06801e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the best model\n",
    "best_model = trained_models['Support Vector Classifier']['trained']\n",
    "\n",
    "# saving the best classification model\n",
    "import joblib\n",
    "joblib.dump(best_model,open('Malignant_comments_classifier.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ee620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the best classification model\n",
    "import joblib\n",
    "best_model = joblib.load('Malignant_comments_classifier.pkl')\n",
    "#joblib.dump(best_model,open('Malignant_comments_classifier.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12097893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and view the results\n",
    "predict_test = best_model.predict(test_vec.toarray())\n",
    "\n",
    "# Saving predicted values into a CSV file\n",
    "pd.DataFrame(predict_test).to_csv('Predicted_test_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac4449",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Predicted_test_output.csv')\n",
    "df1.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df1.rename({'0':'malignant', '1':'highly_malignant', '2':'rude', '3':'threat', '4':'abuse', '5':'loathe'}, \n",
    "           axis='columns', inplace=True)\n",
    "df2=df_test.copy()\n",
    "df = pd.concat([df2, df1], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f308b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test_dataset_predictions2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c744cc69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625d2f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be30b413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5ee7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c1af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42a5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
